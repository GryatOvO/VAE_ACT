{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b739b78d-8112-4307-9eac-329901e43ae8",
   "metadata": {},
   "source": [
    "# Mobile_ALOHA:从VAE到ACT\n",
    "\n",
    "\n",
    "## 从GAN, VAE到扩散模型diffusion model：\n",
    "\n",
    "    VAE常作为一种生成模型, 此外还有生成对抗网络GAN，以及现在非常火热的扩散模型diffusion model等。GAN中，生成器和判别器通过一个对抗过程进行训练。生成器试图生成逼真的数据样本，而判别器则试图区分生成的数据和真实数据。生成器的目标是欺骗判别器，使其认为生成的数据是真实的；判别器的目标是准确地区分真实数据和生成数据。这个过程可以看作是一个两者之间的博弈。事实上，因为GAN的目标函数就是用来以假乱真的，所以GAN生成的图片的保真度非常高，但其需要同时对抗性训练两个网络，不够稳定，而且创造性不够，且整个训练过程所有输出来自于网络，是隐式的，并不是一个概率模型，可解释性不高，在数学上不如后续的AE，DAE，VAE优美。\n",
    "    \n",
    "    接着让我们来看AutoEncoder（AE），以及后来的VAE。AE相对简单，也是很早之前的技术了，大概就是：给定一个输入X，经过一个Encoder，得到一个向量（bottleneck），然后这个bottleneck再输入给一个Decoder，试图去重建输入的X,因为是X自己重建自己，有一种自回归的意味，所以叫自编码器（AutoEncoder）。紧接着出来一个denosing auto-encoder（DAE），就是把输入的原图X进行了一定程度的打乱，再把扰乱过后的Xc（corrupted X)输入到encoder，后续与AE一样，在最后我们希望输出的X依然能够重建原始输入的X，而不是扰乱过后的Xc，这个改进被证明非常有用。\n",
    "![Image](./1.jpeg)\n",
    "\n",
    "    但是不论是AE还是DAE还是MAE，他们的主要目的都是去学中间的这个bottleneck特征向量Z，然后拿这个特征去做一些分类，检测，分割的任务，而不是用来做生成的，因为其实它学到的不是一个概率分布，我们没法对他进行采样，也就是这里的Z，它并不是像GAN的一样是一个随机噪声，而是一个专门用来重建的一个特征，但是这种Encoder_Decoder是一种很好的结构，那问题就是我们如何使用这种结构去做图像生成呢？那么我们就有了VAE变自分编码器，Variational Auto_Encoder.\n",
    "![Image](./2.jpg)\n",
    "\n",
    "    VAE和AE其实是非常不一样的，虽然它的整体框架看起来差不多，然后它的目标函数还是让最后的输出去尽量重建输入的X，但是重要的区别在于，它的中间不再是学习一个固定的bottleneck特征向量，而是一个分布，在这里作者假设它服从一个高斯分布（原因后面会说到），在这里我们encoder就是一些FC层，然后去预测这个高斯分布的均值和方差，那么我们的Z的分布就可以根据上面的公式从得出，之后我们就可以从这个分布中进行采样并输入Decoder，也就是说，当我们的模型训练好之后，你完全可以前面的这个Encoder直接扔掉，将采样到的Z放入Decoder，得到输出，这就可以来做图像生成了。因为VAE预测的是一个分布，从贝叶斯概率的角度来看，前面这个给定X得到Z的过程，就是一个后验概率，然后学出来的distribution就是一个先验分布，那对于decoder部分，对于给定的Z，去预测一张图片X，其实就是似然，那么目标函数我们就是要做一个最大似然估计，从数学上看，就是非常干净优美。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e9f97-b9a7-4e23-9328-bb6451be9be4",
   "metadata": {},
   "source": [
    "## VAE的直觉理解\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223f7a5-7f27-4ca0-b7e2-2e5a935eec49",
   "metadata": {},
   "source": [
    "## 证据下界ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60560b78-cb0f-4bef-bea5-d70f19575ee9",
   "metadata": {},
   "source": [
    "## 目标函数 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcf6e3-fb8f-4961-8496-6c8727a5c194",
   "metadata": {},
   "source": [
    "## 重参数化 Reparameterization\n",
    "\n",
    "![Image](./reparameter.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa05ad29-937d-4bf1-99bf-6fbe4626999d",
   "metadata": {},
   "source": [
    "## 损失函数 Loss function\n",
    "\n",
    "![image](./loss.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed520ebb-2593-46bd-b9c7-994d84159b76",
   "metadata": {},
   "source": [
    "## VAE 效果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b64df-8b9c-499c-bc1b-fc123df47a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784adef8-6695-467d-9cf0-b2fae33f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "\n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61f4794-9f9b-4171-8c34-28356fdac444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += model.loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(), 'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003a6184-4c1b-4e72-bb70-121ccb00ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 9912422/9912422 [00:10<00:00, 919782.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 28881/28881 [00:00<00:00, 150867.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1648877/1648877 [00:01<00:00, 900885.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 1100614.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.596191\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 177.746704\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 154.853241\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 139.441666\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 132.236008\n",
      "====> Epoch: 1 Average loss: 164.8239\n",
      "====> Test set loss: 128.4906\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 129.827499\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 128.262863\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 123.591225\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 122.722702\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 119.248871\n",
      "====> Epoch: 2 Average loss: 122.2261\n",
      "====> Test set loss: 116.3133\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 112.561386\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 108.714340\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.825851\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 110.753777\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 116.306938\n",
      "====> Epoch: 3 Average loss: 114.9346\n",
      "====> Test set loss: 112.4675\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 111.102585\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.929337\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 110.681404\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 115.239975\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 111.130302\n",
      "====> Epoch: 4 Average loss: 111.7245\n",
      "====> Test set loss: 109.9731\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 115.200195\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 109.943008\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 108.662132\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 110.236557\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 111.708984\n",
      "====> Epoch: 5 Average loss: 109.8655\n",
      "====> Test set loss: 108.4085\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 110.731972\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 109.707794\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 107.963501\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 110.131783\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 110.689590\n",
      "====> Epoch: 6 Average loss: 108.6871\n",
      "====> Test set loss: 107.5360\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 103.080887\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 104.728638\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 106.435150\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.778793\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 107.420273\n",
      "====> Epoch: 7 Average loss: 107.8806\n",
      "====> Test set loss: 107.0914\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.017052\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 110.509064\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 105.813118\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.161682\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 107.387489\n",
      "====> Epoch: 8 Average loss: 107.1971\n",
      "====> Test set loss: 106.3540\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 109.184975\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 105.374809\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 109.819901\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 101.471024\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.149849\n",
      "====> Epoch: 9 Average loss: 106.6378\n",
      "====> Test set loss: 105.9332\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 108.384575\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 107.424339\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 111.631744\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 106.289368\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 106.333488\n",
      "====> Epoch: 10 Average loss: 106.2310\n",
      "====> Test set loss: 105.7030\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "def sample(model, epoch):\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'sample_' + str(epoch) + '.png')\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "model = VAE(input_dim=784, hidden_dim=400, latent_dim=20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    #sample(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27d204-c003-4bec-84aa-9ecf5ee53667",
   "metadata": {},
   "source": [
    "第1轮训练后重建的图像效果\n",
    "![Image](./reconstruction_1.png)\n",
    "\n",
    "第10轮训练后重建的图像效果\n",
    "![Image](./reconstruction_10.png)\n",
    "\n",
    "可以看出模型经过训练后重建的图像更加接近输入的原图像，且loss有收敛的趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b96e8dd-b516-4fcf-a19c-c6fe5efc34d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample(model,epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "sample(model,epochs) #对训练完的模型decoder构造随机的输入进行采样生成来查看效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9edd0-6b53-4d8b-af94-8c4ff4714a5d",
   "metadata": {},
   "source": [
    "![Image](./sample_10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a742c9-3790-4f18-a247-d755b508743a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pylearning_env",
   "language": "python",
   "name": "pylearning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
